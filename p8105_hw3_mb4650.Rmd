---
title: "Homework 4"
author: Maya Bunyan
output: github_document
---

This my solution to Homework 4!

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


# Problem 1

Load instacart dataset.

```{r}
data(instacart)
```

The dataset comes from the Instacart online grocery service. It includes `r nrow(instacart)` observations for `r ncol(instacart)` variables. These variables include the order, user, and product ID. There is also information about the order products were added to the cart, if the products were ordered by the user before, as well as when the order was placed. Information regarding the product, including the name, what aisle it is on, and what department it is in. Each observation in the dataset includes this information for one order for a given user.

How many aisles and which aisles are the most items ordered from?

```{r}
instacart %>%
count(aisle) %>%
arrange(desc(n))
```

Make a plot!

```{r}
instacart %>%
count(aisle) %>%
filter(n > 10000) %>%
mutate(
aisle = factor(aisle),
aisle = fct_reorder(aisle, n)
) %>%
ggplot(aes(x = aisle, y = n)) +
geom_point() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Table of three most popular items in given aisles.

```{r}
instacart %>%
filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
group_by(aisle) %>%
count(product_name) %>%
mutate(rank = min_rank(desc(n))) %>%
filter(rank < 4) %>%
arrange(aisle, rank) %>%
knitr::kable()
```


Table mean time Apples and Ice Cream were ordered.

```{r}
instacart %>%
filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
group_by(product_name, order_dow) %>%
summarize(mean_hour = mean(order_hour_of_day)) %>%
pivot_wider(
names_from = order_dow,
values_from = mean_hour
)
```


## Problem 2

Load and tidy accel data.

```{r}
accel_tidy_data =
  read_csv(
    "./accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count")%>%
  mutate(
    day = as.factor(day),
    minute = as.numeric(minute),
    week_day_end = ifelse(day == "Saturday" | day == "Sunday", "Weekdend", "Weekday")
  )

```

This dataset includes `r nrow(accel_tidy_data)` observations and `ncol(accel_tidy_data)` variables. The variables include information on what week and day the data is from, the day id, the minute of the day the activity was taking place, as well as the count of the activity taking place in that minute.

Table total activity for day.

```{r}
accel_tidy_data %>%
    mutate(
      day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
    ) %>%
   group_by(week, day) %>%
  summarize(total_activity = sum(activity_count))

```


From the output table, I do not notice any trends in the total activity counts.

Create plot activity over 24 hours.

```{r}
accel_tidy_data %>%
  ggplot(aes(x = minute, y = activity_count, color = day, group_by(day_id))) +
  geom_line()
```

From the graph we are better able to see trends. We can see that there are spikes in activity at certain parts of the day, specifically around minute 500-800 and 1200-1400. 


## Problem 3

Load the data.

```{r}
data("ny_noaa")
```


Clean the dataset.

```{r}
ny_noaa_tidy = 
  ny_noaa %>%
    separate(
    col = date, 
    into = c("year", "month", "day"),
    convert = TRUE) %>%
  mutate(
    tmax = as.numeric(tmax),
    tmax = tmax/10,
    tmin = as.numeric(tmin),
    tmin = tmin/10
  )

count = 
  ny_noaa_tidy %>% 
  count(snow) %>%
  arrange(desc(n))%>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4)


```

For snowfall, the most common values are 0 and NA. This makes sense since there is no snowfall for most of the year in NY. 

Create two-panel plot.

```{r}
ny_noaa_tidy %>%
  select(id, month, year, tmax) %>%
  filter(month == 7 | month == 1) %>%
  group_by(id, month, year)%>%
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax)) + 
  geom_point(alpha = 0.5) +
  facet_grid(~month) +
  theme(legend.position = "bottom")

```

The mean max temps for January appears to be lower (between -20 and 10 C) than that of the mean max temps for July (between 20 and 40 C). We see an outlier around 1982 in January and another around 2005. There appears to be an outlier around 1988 in July. 

```{r}
tmax_tmin = 
  ny_noaa_tidy %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_smooth(se = FALSE) +
  theme(legend.position = "bottom")

snow_dens = 
  ny_noaa_tidy %>% 
  filter(0 < snow & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) + 
  geom_violin(aes(fill = year)) + 
  theme(legend.position = "bottom")

tmax_tmin + snow_dens
```

